{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBrh2jMpkJ+R5H5LyFMtPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austinlasseter/pca_word_vectors/blob/main/starter_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHoc6hTNz0P"
      },
      "source": [
        "## Word Vectors and Principle Components Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h7tsVB6N6ou"
      },
      "source": [
        "# import the PCA module from sklearn\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPXzaNx-QaiH"
      },
      "source": [
        "Word vectors represent a significant leap forward in advancing our ability to analyze relationships across words, sentences, and documents. It is word vectors that make technologies such as speech recognition and machine translation possible. **Word vectors are simply vectors of numbers that represent the meaning of a word.**   \n",
        "http://jalammar.github.io/illustrated-word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5QG9vg9RdpS"
      },
      "source": [
        "![](https://github.com/austinlasseter/pca_word_vectors/blob/main/word2vec.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6L2u2u4Rr3M"
      },
      "source": [
        "This is a word embedding for the word “king” (GloVe vector trained on Wikipedia):\n",
        "```\n",
        "[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n",
        "```\n",
        "\n",
        "It’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2). \n",
        "\n",
        "See how “Man” and “Woman” are much more similar to each other than either of them is to “king”? This tells you something. These vector representations capture quite a bit of the information/meaning/associations of these words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2gBNxGlQXmE"
      },
      "source": [
        "![](https://github.com/austinlasseter/pca_word_vectors/blob/main/king-man-woman-embedding.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrw9bsnqSGm8"
      },
      "source": [
        "Now let's try working with some word vectors of our own, and seeing how we can combine them with a technique called Principle Components Analysis (PCA) in order to learn something about their meanings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdgCiNMzOm4R"
      },
      "source": [
        "# read in a dataset of 10 word vectors (this is the result of an NLP using Word2Vec)\n",
        "url ='https://raw.githubusercontent.com/austinlasseter/learnspacy/master/words300df.csv'\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-VTYz0POqfJ"
      },
      "source": [
        "# create the index of the dataframe\n",
        "words = ['car', 'truck', 'suv', 'elves', 'dragon', 'sword', 'king', 'queen', 'prince',  'potato']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf_29KrqO-4P"
      },
      "source": [
        "# Take a look at the df\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGO7JKNPAdt"
      },
      "source": [
        "Why do the 2 most popular neural models — Word2Vec and GloVe - consistently use 300-D word vectors?\n",
        "\n",
        "https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "\n",
        "\"having a lower number of parameters leads to better generalization. It is found that 300-dimensional word embeddings perform much better than, say, 3000-dimensional ones.\"\n",
        "https://medium.com/explorations-in-language-and-learning/understanding-word-vectors-f5f9e9fdef98"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMgcKPGPC7E"
      },
      "source": [
        "# let's look at our first word and its 300D word vectors\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5WAkHdfS3VT"
      },
      "source": [
        "# intialise the pca model \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y9-ujBjS5tn"
      },
      "source": [
        "# fit the pca model to our 300-dimensional data, this will work out which is the best \n",
        "# way to project the data down that will best maintain the relative distances \n",
        "# between data points. It will store these intructions on how to transform the data.\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q3N_pIGTAOt"
      },
      "source": [
        "# Tell our (fitted) pca model to transform our 300D data down onto 2D using the \n",
        "# instructions it learnt during the fit phase.\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn3VHKUbTCYB"
      },
      "source": [
        "# let's take look at our first word and its new 2D word vectors\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0LSlNCJTFrZ"
      },
      "source": [
        "# each word and vector pair are actually just X-Y coordinates\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAH6hqDmTMax"
      },
      "source": [
        "# Make that into a dataframe\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kkz_zrRTNYH"
      },
      "source": [
        "# create a  plot \n",
        "# plt.figure(figsize=(10,5))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLe7fYizk5Lq"
      },
      "source": [
        "## in 3 Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB9-R1ouTSMi"
      },
      "source": [
        "Plotly graph\n",
        "Plotly website: https://plot.ly/python/\n",
        "Plotly Forum: https://community.plot.ly/\n",
        "Github repository: https://github.com/austinlasseter/plotly_dash_tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t7nbqP2TQb7"
      },
      "source": [
        "import plotly\n",
        "import plotly.graph_objs as go"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPd0D8jFTWOB"
      },
      "source": [
        "# for colab notebooks:\n",
        "def enable_plotly_in_cell():\n",
        "    import IPython\n",
        "    from plotly.offline import init_notebook_mode\n",
        "    display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
        "    init_notebook_mode(connected=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4LtW3hdTbnP"
      },
      "source": [
        "# intialise pca model for 3 dimensions\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu67jY-xTdmk"
      },
      "source": [
        "# fit the pca model to our 300-dimensional data\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhcMk9oyTfH1"
      },
      "source": [
        "# Tell our (fitted) pca model to transform our 300D data down onto 2D using the \n",
        "# instructions it learnt during the fit phase.\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8RrcPemTjA9"
      },
      "source": [
        "# each word and vector pair are coordinates\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUUjS9nZTkZv"
      },
      "source": [
        "# convert to a 3D dataframe\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izOJQGsITl7n"
      },
      "source": [
        "# for each word and coordinate pair: \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxB5qRHATnxR"
      },
      "source": [
        "# now with plotly create a dictionary of values using the 'go.Scatter' class\n",
        "mydata = go.Scatter3d(x = x_values, \n",
        "                      y = y_values, \n",
        "                      z = z_values, \n",
        "                      mode='markers', \n",
        "                      hovertext=words,\n",
        "                      marker=dict(\n",
        "                            size=12,\n",
        "                            color=z_values,                # set color to an array/list of desired values\n",
        "                            colorscale='Viridis',   # choose a colorscale\n",
        "                            opacity=0.8\n",
        "                        )\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ARkUj4Tp5O"
      },
      "source": [
        "enable_plotly_in_cell() # don't forget to include this! It's just for colab notebooks.\n",
        "# wrap that dictionary into a list and display using 'go.Figure' class\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}